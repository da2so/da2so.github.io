---
layout: post
title: Interpretable And Fine-grained Visual Explanations For Convolutional Neural Networks
tags: [Explainable AI(XAI), Perturbation, Interpretable, Fine-grained]
comments: true
use_math: true
thumbnail-img: /assets/thumbnail_img/2020-08-17-Interpretable_And_Fine-grained_Visual_Explanations_For_Convolutional_Neural_Networks/post.png
---

## 1. Goal

We propose an optimization based visual explanation method, which highlights the evidence in the input images for a specific prediction.


### <span style="color:gray">1.1 Sub-goal </span>

<span style="color:gray">*A.*</span> Defend against adversarial evidence (i.e. faulty evidence due to artifacts).
<span style="color:gray">*B.*</span> Provide the explanations which are both fine-grained and preserve the characteristics of images, such as edges and colors.

![1](https://da2so.github.io/assets/post_img/2020-08-17-Interpretable_And_Fine-grained_Visual_Explanations_For_Convolutional_Neural_Networks/1.png){: .mx-auto.d-block :}


## 2. Method

We provide *local explanations*, which focus on an individual input. Given one data point, our method highlights the evidence on which a model bases its deciion.


### <span style="color:gray">2.1 Perturbation based visual explanations </span>

#### Perturbatuion based explanations can be defined as: 
*  <span style="color:#5256BC">Explanation by preservation: </span> The smallest region of the image which must be retained to preserve the original model output. 
*  <span style="color:#5256BC">Explanation by deletion: </span> The smallest region of the image which must be deleted to change the model output.


#### <span style="color:gray">2.1.1 Problem definition </span>

* CNN: <span style="color:DodgerBlue">$f_{cnn}$</span>
* Input image: <span style="color:DodgerBlue">$x \in \mathbb{R}^{3 \times H \times W}$</span>
* Output: <span style="color:DodgerBlue">$y_x=f_{cnn} (x; \theta_\{ cnn\} )$</span>
* Softmax scores:  <span style="color:DodgerBlue">$y^c_x$</span> of the different classes <span style="color:DodgerBlue">$c$</span>
* Explanations: <span style="color:DodgerBlue">$e^x_{c_T}$</span> for a target class <span style="color:DodgerBlue">$c_T$</span>


#### <span style="color:gray">2.1.2 The objective function </span>

An explanation is computed by removing either relevant or irrelevant information from the image <span style="color:DodgerBlue">$x$</span>. 

To do this, we use a mask based operator <span style="color:DodgerBlue">$\Phi$</span>, which computes a weighted average between the image <span style="color:DodgerBlue">$x$</span> and a reference image <span style="color:DodgerBlue">$r$</span>, using a mask <span style="color:DodgerBlue">$m_{c_T} \in \[ 0,1 \]^{3 \times H \times W}$</span>:


![1](https://da2so.github.io/assets/post_img/2020-08-17-Interpretable_And_Fine-grained_Visual_Explanations_For_Convolutional_Neural_Networks/2.png){: .mx-auto.d-block  width="82%" :}

<!--
<span style="color:DodgerBlue">
\\[
e_{c_T}=\Phi(x,m_\{c_T \})= x \cdot m_\{c_T \} + (1- m_\{c_T \} )\cdot r \quad \cdots Eq.(1).
\\]
</span>
-->

We introduce a similarity metric <span style="color:DodgerBlue">$\varphi (y^{c_T}_x, y^{c_T}_e)$</span>.

* Measruing the consistency of the model output generated by the explanation <span style="color:DodgerBlue">$y^{c_T}_e$</span> and the output of the image <span style="color:DodgerBlue">$y^{c_T}_x$</span> with respect to a target class <span style="color:DodgerBlue">$c_T$</span>

* Typical chocies for the metric (i) cross-entropy and (ii) negative softmax score of the target class


|**If** the explanation preserves the output of <span style="color:DodgerBlue">$c_T$</span> $\rightarrow$ the similarity is small|
|**Elif** the explanation manages to significantly drop the probability of <span style="color:DodgerBlue">$c_T$</span> $\rightarrow$ the similarity is large|


From the similarity metric, we define two versions of the objective functions.

(1) Preserving game

Using the mask based definition of an explanation with a reference (<span style="color:DodgerBlue">$r=0$</span>) as well as similarity metric, a *preserving explanation* can be computed by:

<span style="color:DodgerBlue">
\\[
e^{ \ast }_{ c_T } = m^{ \ast }_{ c } 
\\]
</span>

where <span style="color:DodgerBlue">$\lambda$</span> encourages the mask to be sparse (*i.e.* many pixels are zero / appear black).

(2) Deletion game

we can compute a *deleting explanation* using:



<span style="color:DodgerBlue">$\lambda$</span> encourages masks to contain mainly ones (*i.e.* appear white) such as Fig. 3 (b1).



![3](https://da2so.github.io/assets/post_img/2020-08-17-Interpretable_And_Fine-grained_Visual_Explanations_For_Convolutional_Neural_Networks/3.png){: .mx-auto.d-block :}


To solve the optimization in Eq. (2) and (3), we utilize Stochastic Gradient Descent (SGD and start with an explanation <span style="color:DodgerBlue">$e^0_{c_T} =1 \cdot x$</span> identical to the original image (*i.e.* a mask initialized with ones).