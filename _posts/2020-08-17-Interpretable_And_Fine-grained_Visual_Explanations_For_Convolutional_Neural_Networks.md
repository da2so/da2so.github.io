---
layout: post
title: Interpretable And Fine-grained Visual Explanations For Convolutional Neural Networks
tags: [Explainable AI(XAI), Perturbation, Interpretable, Fine-grained]
comments: true
use_math: true
thumbnail-img: /assets/thumbnail_img/2020-08-17-Interpretable_And_Fine-grained_Visual_Explanations_For_Convolutional_Neural_Networks/post.png
---

## 1. Goal

We propose an optimization based visual explanation method, which highlights the evidence in the input images for a specific prediction.


### <span style="color:gray">1.1 Sub-goal </span>

<span style="color:gray">*A.*</span> Defend against adversarial evidence (i.e. faulty evidence due to artifacts).
<span style="color:gray">*B.*</span> Provide the explanations which are both fine-grained and preserve the characteristics of images, such as edges and colors.

![1](https://da2so.github.io/assets/post_img/2020-08-17-Interpretable_And_Fine-grained_Visual_Explanations_For_Convolutional_Neural_Networks/1.png){: .mx-auto.d-block :}


## 2. Method

We provide *local explanations*, which focus on an individual input. Given one data point, our method highlights the evidence on which a model bases its deciion.


### <span style="color:gray">2.1 Perturbation based visual explanations </span>

#### Perturbatuion based explanations can be defined as: 
*  <span style="color:#5256BC">Explanation by preservation: </span> The smallest region of the image which must be retained to preserve the original model output. 
*  <span style="color:#5256BC">Explanation by deletion: </span> The smallest region of the image which must be deleted to change the model output.


#### <span style="color:gray">2.1.1 Problem definition </span>

* CNN: <span style="color:DodgerBlue">$f_{cnn}$</span>
* Input image: <span style="color:DodgerBlue">$x \in \mathbb{R}^{3 \times H \times W}$</span>
* Output: <span style="color:DodgerBlue">$y_x=f_{cnn} (x; \theta_\{ cnn\} )$</span>
* Softmax scores:  <span style="color:DodgerBlue">$y^c_x$</span> of the different classes <span style="color:DodgerBlue">$c$</span>
* Explanations: <span style="color:DodgerBlue">$e^x_{c_T}$</span> for a target class <span style="color:DodgerBlue">$c_T$</span>


#### <span style="color:gray">2.1.2 The objective function </span>

An explanation is computed by removing either relevant or irrelevant information from the image <span style="color:DodgerBlue">$x$</span>. 

To do this, we use a mask based operator <span style="color:DodgerBlue">$\Phi$</span>, which computes a weighted average between the image <span style="color:DodgerBlue">$x$</span> and a reference image <span style="color:DodgerBlue">$r$</span>, using a mask <span style="color:DodgerBlue">$m_{c_T} \in \[ 0,1 \]^{3 \times H \times W}$</span>:


![1](https://da2so.github.io/assets/post_img/2020-08-17-Interpretable_And_Fine-grained_Visual_Explanations_For_Convolutional_Neural_Networks/2.png){: .mx-auto.d-block  width="82%" :}

<!--
<span style="color:DodgerBlue">
\\[
e_{c_T}=\Phi(x,m_\{c_T \})= x \cdot m_\{c_T \} + (1- m_\{c_T \} )\cdot r \quad \cdots Eq.(1).
\\]
</span>
-->

We introduce a similarity metric <span style="color:DodgerBlue">$\varphi (y^{c_T}_x, y^{c_T}_e)$</span>.

* Measruing the consistency of the model output generated by the explanation <span style="color:DodgerBlue">$y^{c_T}_e$</span> and the output of the image <span style="color:DodgerBlue">$y^{c_T}_x$</span> with respect to a target class <span style="color:DodgerBlue">$c_T$</span>

* Typical chocies for the metric (i) cross-entropy and (ii) negative softmax score of the target class


|**If** the explanation preserves the output of <span style="color:DodgerBlue">$c_T$</span> $\rightarrow$ the similarity is small|
|**Elif** the explanation manages to significantly drop the probability of <span style="color:DodgerBlue">$c_T$</span> $\rightarrow$ the similarity is large|


From the similarity metric, we define two versions of the objective functions.

<span style="color:#5256BC">**(1) Preserving game**</span>

Using the mask based definition of an explanation with a reference (<span style="color:DodgerBlue">$r=0$</span>) as well as similarity metric, a *preserving explanation* can be computed by:

<span style="color:DodgerBlue">
\\[
\\begin{array}{l} e^\ast_\{ c_T \} = m^\ast_\{ c_T \} \cdot x, \cr
				  m^\ast_\{ c_T \}= argmin_\{ m_\{ c_T \} \}
\\end{array}.
\\]
</span>

where <span style="color:DodgerBlue">$\lambda$</span> encourages the mask to be sparse (*i.e.* many pixels are zero / appear black).

<span style="color:#5256BC">**(2) Deletion game**</span>

we can compute a *deleting explanation* using:



<span style="color:DodgerBlue">$\lambda$</span> encourages masks to contain mainly ones (*i.e.* appear white) such as Fig. 3 (b1).



![3](https://da2so.github.io/assets/post_img/2020-08-17-Interpretable_And_Fine-grained_Visual_Explanations_For_Convolutional_Neural_Networks/3.png){: .mx-auto.d-block :}


To solve the optimization in Eq. (2) and (3), we utilize Stochastic Gradient Descent (SGD and start with an explanation <span style="color:DodgerBlue">$e^0_{c_T} =1 \cdot x$</span> identical to the original image (*i.e.* a mask initialized with ones).



### <span style="color:gray">2.2 Defending against Adversarial Evidence </span>

CNNs have been proven susceptible to adversarial images. Due to the computational similarity of adversarial methods and optimization based visual explanation approaches, advesarial noise is aslo a concern for our method. 

To tackle this problem, we propose a novel adversarial defense which filters gradients during backpropagation in a targeted way. The basic idea is: A neuron within a CNN is only allowed to be activated by the explanation <span style="color:DodgerBlue">$e_{c_T}$</span> if the same neuron was also activated by the original image <span style="color:DodgerBlue">$x$</span>.  


If we regard neurons as indicators for the existence of features (*e.g.* edges, object parts, ...), the proposed constraint enforces that the explanation <span style="color:DodgerBlue">$e_{c_T}$</span> can only contain features which exist at the same location in the original image <span style="color:DodgerBlue">$x$</span> as follow:

<span style="color:DodgerBlue">
\\[ 
\\left\\{ \\begin{array}{ll} 0 \leq h^l_i (e_{c_T}) \leq h^l_i (x), & if \; h^l_i (x) \geq 0, \cr
							 0 \geq h^l_i (e_{c_T}) \geq h^l_i (x), & otherwise, 
							\\end{array} \\right. 
\\]</span>

where <span style="color:DodgerBlue">$h^l_i$</span> is the activation of the <span style="color:DodgerBlue">$i$</span>-th neuron in the <span style="color:DodgerBlue">$l$</span>-th layer of the network after the nonlinearity. This constraint is applied after all nonlinearity-layers (*e.g.* ReLU-Layers) of the network, besides the final final classification layer.


To solve the optimization with subject to Eq. (4), one could incorporate the constraints via a penalty function with adding an additional layer <span style="color:DodgerBlue">$\overline{h}^l_i$</span> after each nonlinearity:

\\[
\overline{h}^l_i (e_{c_T})= min (bu, max(bl, h^l_i (e_{c_T}))), \cr
bu =max(0,h^l_i (x)), \cr
bl= min (0, h^l_i (x)),
\\]


where <span style="color:DodgerBlue">$h^l_i(e_{c_T})$</span> is the acutal activation of the origianl nonlinearity-layer and <span style="color:DodgerBlue">$\overline{h}^l_i (e_{c_T})$</span> the adjusted activation after ensuring the bounds <span style="color:DodgerBlue">$bu00$</span>,<span style="color:DodgerBlue">$bl$</span> of the original input. 